---
title: "Tracing counterfactual development"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    theme: cosmo
    highlight: tango
---

# Load packages

```{r, warning=F, message=F}
library("janitor")      # clean up column names
library("knitr")        # for knitting stuff
library("kableExtra")   # for markdown tables
library("xtable")       # for latex tables
library("broom")        # for tidy model fits
library("ggeffects")    # for marginal effects
library("BayesFactor")  # for calculating Bayes factors
library("TreeBUGS")     # for estimating the hierachical Bayesian MPT model
library("tidybayes")    # for extracting parameters from Bayesian model
library("ggridges")     # for ridgeline plot
library("MPTinR")       # for calculating MPT predictions
library("afex")         # for mixed model analysis of Exp. 2
library("tidyverse")    # everything else
```

```{r}
theme_set(theme_classic() +
            theme(text = element_text(size = 24)))

opts_chunk$set(comment = "#>",
               fig.show = "hold")
```

# Helper functions 

```{r}
print_table = function(data, format = "html", digits = 2){
  if(format == "html"){
    data %>% 
      kable(digits = digits) %>% 
      kable_styling()
  }else if(format == "latex"){
    data %>% 
      xtable(digits = digits) %>%
      print(include.rownames = F,
            booktabs = T)
  }
}
```

# Experiment 1a  

## Read in data 

```{r, message=F}
df.exp1a = read_csv("../data/experiment1a_data.csv") %>% 
  clean_names() %>% 
  filter(age_group != 4) %>% # remove 4 year old children from pilot 
  mutate(age_group = factor(age_group)) %>% 
  rename(condition = cond)
```

## Demographics

```{r, eval=FALSE}
df.exp1a %>% 
  filter(gng == 1) %>% 
  group_by(age_group, sex) %>%
  summarize(count = n()) %>% 
  group_by(age_group) %>% 
  mutate(n = sum(count)) %>% 
  pivot_wider(names_from = sex, 
              values_from = count) %>% 
  print_table()

df.exp1a %>% 
  filter(gng == 0) %>% 
  group_by(age_group, sex) %>%
  summarize(count = n()) %>% 
  group_by(age_group) %>% 
  mutate(n = sum(count)) %>% 
  pivot_wider(names_from = sex, 
              values_from = count) %>% 
  print_table()
```

## Stats 

### Overall logistic regression 

```{r}
fit = glm(formula = resp_acc ~ age_group * condition,
          family = "binomial",
          data = df.exp1a %>% 
            filter(gng == 1))

fit %>% 
  tidy() %>% 
  print_table()
```

### Logistic regression in overdetermined condition 

```{r}
fit = glm(formula = resp_acc ~ age_group,
          family = "binomial",
          data = df.exp1a %>% 
            filter(gng == 1,
                   condition == "D"))

fit %>% 
  tidy() %>% 
  print_table()
```

### Binomial exact test 

```{r}
binom.test(x = df.exp1a %>% 
             filter(gng == 1,
                    condition == "D") %>% 
             summarize(sum = sum(resp_acc)) %>% 
             pull(sum),
           n = df.exp1a %>% 
             filter(gng == 1,
                    condition == "D") %>% 
             nrow(.)) %>% 
  tidy() %>% 
  print_table()
```

## Plots

```{r fig.height=6, fig.width=10, warning=F}
df.plot = df.exp1a %>% 
  mutate(age_group = factor(age_group,
                            levels = c("5", "7", "9"),
                            labels = c("5-6\nyears", "7-8\nyears","9-10\nyears")),
         condition = factor(condition,
                            levels = c("C", "D"),
                            labels = c("makes difference", "overdetermined"))) %>% 
  filter(gng == 1)

df.text = df.plot %>% 
  count(age_group, condition) %>% 
  mutate(n = str_c("n = ", n),
         resp_acc = 1.05)

ggplot(data = df.plot,
       mapping = aes(x = age_group,
                     y = resp_acc,
                     group = condition,
                     fill = condition)) + 
  stat_summary(fun = "mean",
               geom = "bar",
               color = "black",
               position = position_dodge(width = 0.9)) + 
  stat_summary(fun.data = "mean_cl_boot",
               geom = "linerange",
               color = "black",
               position = position_dodge(width = 0.9),
               size = 1) + 
  geom_text(data = df.text,
            mapping = aes(label = n),
            position = position_dodge(width = 0.9),
            size = 8) + 
  geom_hline(yintercept = 0.5,
             linetype = 2) +
  labs(x = "age group",
       y = "accuracy") + 
  scale_fill_brewer(palette = "Set1") + 
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     labels = str_c(seq(0, 100, 25), "%"),
                     expand = expansion(mult = c(0, .03))) + 
  theme(text = element_text(size = 36),
        legend.position = "bottom",
        axis.title.x = element_blank(),
        legend.title = element_blank())

ggsave("../figures//experiment1a_bars.pdf",
       width = 10,
       height = 6)
```

# Experiment 1b 

## Read in data 

```{r, message=F}
df.exp1b = read_csv("../data/experiment1b_data.csv") %>% 
  clean_names() %>% 
  filter(gng == 1)
```

## Demographics 

```{r}
df.exp1b %>% 
  count(age_group) %>% 
  print_table()
```


## Stats 

```{r}
df.exp1b %>% 
  group_by(age_group) %>% 
  summarize(n = n(),
            n_correct_disjunctive = sum(disjunctive),
            n_correct_conjunctive = sum(disjunctive)) %>% 
  print_table()
```

# Experiment 2 

## Read in data 

```{r, message=F}
df.exp2 = read_csv("../data/experiment2_data.csv") %>% 
  clean_names() %>% 
  select(-contains("question")) %>% 
  pivot_longer(cols = x1_1v1:x2_3,
               names_to = "question",
               values_to = "answer") %>% 
  mutate(question = str_remove_all(question,"x"),
         answer = factor(answer, 
                         levels = c("C", "DM", "ME", "DI"),
                         labels = c("correct", "match origin", 
                                    "match trajectory", "match neither")),
         question = factor(question, 
                           levels = c("1_1v1", "1_1v2", "1_2", "1_3", 
                                      "2_1v1", "2_1v2", "2_2", "2_3")),
         gender = tolower(gender)) %>% 
  rename(age_group = years) %>% 
  separate(question, into = c("outcome", "type"), remove = F) %>% 
  mutate(outcome_actual = factor(outcome, levels = 1:2, 
                                 labels = c("goal", "miss")),
         outcome_counterfactual = 
           ifelse(type %in% c("1v1", "1v2", "1v1", "1v2"), "same", "different"),
         collision = ifelse(type == "3", "no collision", "collision")) %>% 
  filter(included == 1)
```

## Demographics 

```{r}
df.exp2 %>% 
  distinct(participant, gender, age_group) %>% 
  count(age_group, gender, name = "count") %>% 
  group_by(age_group) %>% 
  mutate(n = sum(count)) %>% 
  pivot_wider(names_from = gender, 
              values_from = count) %>% 
  print_table()
```

## Stats 

### Performance on no-collision cases 

```{r}
df.exp2 %>% 
  filter(collision == "no collision") %>% 
  group_by(age_group) %>% 
  count(answer) %>% 
  group_by(age_group) %>% 
  mutate(proportion = n / sum(n)) %>% 
  filter(answer == "correct") %>% 
  select(age_group, proportion) %>% 
  print_table()
```

### Mixed Model Accuracy Analysis

To analyse accuracy as a function of age group, the actual outcome, and the counterfactual outcome, we first need to prepare a new data set.

```{r}
df2 <- df.exp2 %>% 
  filter(collision == "collision") %>% 
  mutate(age_group = factor(age_group)) %>% 
  group_by(participant, age_group, outcome_actual, outcome_counterfactual) %>% 
  summarise(correct = sum(answer == "correct"), 
            n = n()) %>% 
  mutate(prop = correct/n)
```

We then fit a series of models (or read the fitted model files). `mod0` is the maximal model, `mod3` is the final model.

```{r}
if (!file.exists("cache/mixed_exp2.rda")) {
  mod0 <- mixed(prop ~ age_group*outcome_actual*outcome_counterfactual + 
                  (outcome_actual * outcome_counterfactual||participant), 
                df2, family = "binomial", method = "LRT", weights = n,
                expand_re = TRUE, 
                control = glmerControl(optCtrl = list(maxfun = 1e5)))
  mod1 <- mixed(prop ~ age_group*outcome_actual*outcome_counterfactual + 
                  (outcome_actual + outcome_counterfactual||participant), 
                df2, family = "binomial", method = "LRT", weights = n,
                expand_re = TRUE)
  mod2 <- mixed(prop ~ age_group*outcome_actual*outcome_counterfactual + 
                  (outcome_actual||participant), 
                df2, family = "binomial", method = "LRT", weights = n,
                expand_re = TRUE)
  mod3 <- mixed(prop ~ age_group*outcome_actual*outcome_counterfactual + 
                  (1|participant), 
                df2, family = "binomial", method = "LRT", weights = n,
                expand_re = TRUE)

  save(mod0, mod1, mod2, mod3, file = "fits/mixed_exp2.rda", 
       compress = "xz")
} else {
  load("cache/mixed_exp2.rda")
} 
```

The full model shows a main effect of age group plus an interaction of age group with the counterfactual outcome, but also some convergence warnings (for the full model).
```{r}
mod0
```

We consequently remove the random slope for the interaction. This model shows the same pattern of results, but also convergence warnings:

```{r}
mod1
```

Inspection of the variance estimates shows that the one for the counterfactual outcome is smallest. 

```{r}
summary(mod1)$varcor
```

We remove this in the next step, but the model still shows convergence warnings:

```{r}
mod2
```

We consequently remove the last remaining random slope as well. This still shows some warning, but no more for the full model. The pattern of results remains the same.

```{r}
mod3
```


We can now investigate the interaction of age group and the counterfactual outcome. For this we use the final model.
```{r, fig.width=8, fig.height=4}
afex_plot(mod3, "age_group", "outcome_counterfactual", 
          data_geom = geom_count)
```

This shows an age effect if the counterfactual outcome is the same (i.e., over-determined cases), but not if it is different (i.e., singly-determined cases). This is also confirmed by follow-up tests. First for the final modeL

```{r}
pairs(emmeans::emmeans(mod3, by = "outcome_counterfactual", "age_group"))
```

And for the maximal model:
```{r}
pairs(emmeans::emmeans(mod0, by = "outcome_counterfactual", "age_group"))
```

Do we see an effect of "wishful thinking" such that children in our study were more likely to be correct on trials for which the ball could have gone into the goal but did not (i.e., an actual outcome by counterfactual outcome interaction)? No.

```{r}
emmeans::emmeans(mod3, c("outcome_actual", "outcome_counterfactual"), 
                 type = "response")
```


### MPT analysis 

We first need to prepare the data for fitting. 

```{r, message=FALSE}
age_groups = c(4, 5, 6)
di = df.exp2 %>%
  filter(collision == "collision") %>% # Filter out no-collisions
  mutate(answer = as.numeric(answer)) %>% 
  group_by(participant, months, age_group, answer) %>%
  summarise(count=n()) %>%
  spread(answer,count, fill = 0) %>% 
  ungroup %>% 
  mutate(age2 = as.numeric(substr(age_group, 1, 1)) + months/12) %>% 
  mutate(age = factor(substr(age_group, 1, 1), levels = age_groups))

dc = di %>% 
  select(age) %>% 
  as_tibble()
```


We then fit the full model. Specifically, we fit one model to the data of all age groups, but add `age_group` as a covariate to all three model parameters. This is somewhat time intensive so we save the results (or load the saved results if those exist).

```{r}

if (!file.exists("cache/fit_latent-trait_all.rda")) {
  fit_bayes_all = traitMPT("models/model.eqn", di, restrictions = list("g1=g2=0.5"), 
                            covData = dc,
                            predStructure = list("m_o m_t s ; age"), 
                            predType = "f",
                            n.iter = 320000, n.thin = 300, n.chains = 4,
                            n.adapt = 100000, n.burnin = 20000)
  ppps = PPP(fit_bayes_all)
  save(fit_bayes_all, ppps, file = "fits/fit_latent-trait_all.rda", 
       compress = "xz")
} else {
  load("cache/fit_latent-trait_all.rda")
} 
```

We first check for chain convergence (i.e., Rhat and neff) and get an overview:

```{r}
summary(fit_bayes_all)
```

We also check the trace plots for the (group-level) mean parameters:

```{r, fig.width=5, fig.height=6}
plot(fit_bayes_all, parameter = "mean")
```

The group-level SD parameters (on probit scale):

```{r, fig.width=5, fig.height=6}
plot(fit_bayes_all, parameter = "sigma")
```

The correlation parameters among the individual effects:

```{r, fig.width=5, fig.height=6}
plot(fit_bayes_all, parameter = "rho")
```

And finally the parameters for the effect of age on the model parameter (as well as their SDs).

```{r, fig.width=5, fig.height=6}
plot(fit_bayes_all, parameter = "factor")
```

#### Model Fit

We evaluate model fit using posterior predictive tests. There is no evidence for misfit. All posterior predictive $p$-values are larger than $.05$. This suggests that the model is able to adequately describe the data.

```{r}
ppps
```

This is also visible when plotting the distribution of individual $ppp$-values.

```{r, fig.width=4, fig.height=2}
ggplot(data = as_tibble(ppps$ind.T1.p),
       mapping = aes(value)) +
  geom_histogram(binwidth = 0.05, boundary = 0) +
  coord_cartesian(xlim = c(0, 1)) +
  geom_vline(xintercept = 0.05, linetype = "dashed") +
  xlab("ppp-value")
```

#### Group-Level Estimates

We can investigate the group-level parameter estimates (peaks) and 80% HDIs in tabular form:

```{r}

mus <- gather_draws(fit_bayes_all$runjags, mu[i]) %>% 
    mutate(
      parameter = factor(
        i, 
        levels = c(3, 1, 2), 
        labels = c("italic(s)", "italic(m[o])", "italic(m[t])"))
    ) %>% 
    rename(mean = .value) %>% 
    ungroup() %>% 
    select(-i, -.variable)

pars = gather_draws(fit_bayes_all$runjags, 
                     `factor_.*_age`[i], regex = TRUE) %>% 
  ungroup() %>% 
  mutate(age_group = factor(age_groups[i], level = age_groups),
         #estimate = pnorm(.value),
         parameter = factor(
           str_replace(str_replace(.variable, "factor_", ""), "_age", ""), 
           levels = c("s", "m_o", "m_t"), 
           labels = c("italic(s)", "italic(m[o])", "italic(m[t])"))) %>% 
  mutate(age_group2 = factor(age_group, levels = rev(levels(di$age))))
pars <- left_join(pars, mus) %>% 
    mutate(estimate = pnorm(.value + mean))

pars %>% 
  group_by(parameter, age_group) %>% 
  mode_hdci(estimate, .width = c(0.80)) %>% 
  mutate(.lower = round(.lower, 4))

```


To investigate the difference between age groups, we calculate the difference distribution of the group-level posterior for each pairwise comparison of age groups for each parameter. We first look at the 80% and 95% highest density intervals of the difference distributions. 


```{r, fig.height=3, fig.width=5}
tmp_diff = pars %>% 
  mutate(parameter = factor(parameter, levels = 
                         c("italic(s)", "italic(m[o])", "italic(m[t])"), 
                       labels = c("s", "m_o", "m_t"))) %>% 
  mutate(inter = parameter:age_group) %>% 
  compare_levels(estimate, by = inter) %>% 
  mutate(par1 = str_extract(as.character(inter), "[a-z_]+"),
         par2 = str_extract(substr(as.character(inter), 5, 100), 
                            "[a-z_]+")) %>% 
  filter(par1 == par2) %>% 
  droplevels %>% ungroup %>% 
  mutate(diff = str_remove_all(as.character(inter), "[a-z_:]+")) %>%
  mutate(inter = 
           factor(inter, levels = c(
             "m_o:6 - m_o:4", "m_o:6 - m_o:5", "m_o:5 - m_o:4",  
             "m_t:6 - m_t:4", "m_t:6 - m_t:5", "m_t:5 - m_t:4", 
             "s:6 - s:4", "s:6 - s:5", "s:5 - s:4"))) %>% 
  mutate(diff = 
           factor(diff, levels = c("6 - 4", "6 - 5", "5 - 4"))) %>%
  mutate(parameter = factor(par1, levels = c("s", "m_o", "m_t"), 
           labels = c("italic(s)", "italic(m[o])", "italic(m[t])")))

tmp_diff %>% 
  group_by(inter) %>% 
  mean_hdci(estimate, .width = c(0.80, 0.95))
```

We can also plot the difference distributions plus 80% and 95% credibility intervals. From this it is clear that there is not really any evidence for differences in $m_o$. However, there is some evidence for a difference in $m_t$. Furthermore, the monotonic increase for$s$ also receives some support.

```{r, fig.height=3, fig.width=5}
tmp_diff %>%   
  ggplot(aes(y = diff, x = estimate)) +
  stat_halfeye(.width = c(0.80, 0.95), point_interval = mode_hdci, 
               normalize = "groups") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  facet_wrap(~parameter, labeller = label_parsed) +
  xlab("Difference (Probability Scale)") +
  ylab("Comparison") + 
  theme(text = element_text(size = 12))
```

#### Individual-Level Parameters and Individual Differences

We first inspect the posterior distribution of the sigma parameters.

```{r}

sigmas = gather_draws(fit_bayes_all$runjags, sigma[par]) %>% 
   mutate(parameter = factor(par, 
                             levels = c(3, 1, 2), 
                             labels = c("s", "m_o", "m_t")))
sigmas %>% 
  ggplot(aes(y = parameter, x = .value)) +
  stat_halfeye(.width = c(0.80, 0.95), point_interval = mode_hdci)

sigmas %>%
  group_by(parameter) %>%
  mode_hdci(.value, .width = c(0.80, 0.95))
```


```{r}
samp1 = gather_draws(fit_bayes_all$runjags, theta[i,participant2])

samp1 = di %>% 
  mutate(participant2 = 1:nrow(di)) %>% 
  select(participant2, age_group, age2) %>% 
  right_join(samp1, by = "participant2") %>% 
  mutate(parameter = 
           factor(i, levels = c(3, 1, 2), 
                  labels = c("italic(s)", "italic(m[o])", "italic(m[t])"))) %>% 
  mutate(age_group2 = factor(age_group, 
                             levels = c("4", "5", "6"),
                             labels = c(c("4~years", "5~years", "6~years"))))
```

In addition to the group-level parameters, we can also investigate the distribution of individual-level parameters. For this, we randomly sample 200 of the 4000 posterior samples and plot the density of the individual-level parameter distribution separately per parameter and age group. We plot the individual densities using an alpha-value of 0.05 so that darker regions represent regions with overlap and therefore higher density among the distribution of the individual-level parameters.

```{r, fig.height=4, fig.width=5}
rows = sample.int(max(samp1$.draw), 200)

samp1 %>% 
  filter(.draw %in% rows) %>% 
  ggplot(aes(x = .value,
             group = .draw,
             fill = .value)) +
  stat_density(geom = "line",
               alpha = 0.05,
               adjust = 1, 
               bw = 0.05,
               kernel = "o") +
  facet_grid(age_group2~parameter,
             labeller = label_parsed) +
  scale_x_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), 
                     labels = c("0", "", "0.5", "", "1")) +
  xlab("parameter estimate") +
  theme(text = element_text(size = 16))
```

To ensure this pattern is not an artifact of applying a density estimator individually to each posterior sample, we can simply plot a histogram for these data, which ignore the individual posterior draws. Should this show a very different pattern, this should provide some caution towards the density estimators. However, the pattern looks pretty much the same as when using the density plot.

```{r, fig.height=4, fig.width=5}
samp1 %>% 
  filter(.draw %in% rows) %>% 
  ggplot(aes(x = .value, fill = .value)) +
  geom_histogram(binwidth = 0.05) +
  facet_grid(age_group2 ~ parameter,
             labeller = label_parsed) +
  scale_x_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), 
                     labels = c("0", "", "0.5", "", "1")) +
  xlab("parameter estimate")
```

For $s$, we see that the distributiuons of individual-level parameters are a lot less peaked than for the other two parameters. For the 4-year olds we see a small peak around 0 with the remaining probability mass distributed more towards the left side of the parameter space. For the 5-year olds we see a slightly bimodal pattern, but also some evidence for an almost uniform distribution. For the 6-year olds we see almost a mirror pattern of the 4-year olds with a larger peak at 1 and some evidence for a small peak at 0.

For $m_o$ we can see that the vast majority of individual-level estimates is huddled towards the right boundary of the parameter space. Very few individual-level parameter estimates appear to be below .75. Furthermore, there seem to be few differences between conditions, which is consistent with the pattern of the group-level parameters.

For $m_t$ we see quite a different pattern suggesting a bimodal distribution with the modes being at either ends of the parameter space. Furthermore, the weight given to the two modes seem to shift across age-groups with the 4 and 5-year olds having a larger peak at 0, whereas for the 6-year olds there appears to be a larger peak at 1. This bimodal pattern is consistent with the large SD of the group-level distribution (posterior mean = 7.56), which is also considerably larger than these SDs for the other two parameters (posterior means < 1.5). The reason for this bimodality given a normal group-level distribution is that the latent-trait model assumes a normal distribution on an unconstrained latent space, which is then transformed into the probability space using the probit transformation. 

When comparing the individual-level distributions with the group-level distributions, we see quite a few differences. This suggests that the group-level mean does not necessarily provide a good representation of all individual parameters. One posible reason is parameter trade-offs at the group-level. We investigated this with the next plot.


```{r, fig.height=6.5, fig.width=6.5}
gather_draws(fit_bayes_all$runjags, mu[par], sigma[par]) %>% 
  mutate(parameter = factor(par, levels = 1:3, 
                            labels = c("m_o", "m_t", "s")),
    inter = paste0(.variable, ".", parameter)) %>% 
  ungroup() %>% 
  select(.draw, inter, .value) %>% 
  spread("inter", ".value") %>% 
  select(-.draw) %>% 
  GGally::ggpairs(progress = FALSE, 
                  lower = list(continuous = GGally::wrap("points", alpha = 0.05)))
```

This plot provides little evidence for large parameter trade-offs. We see some mild positive correlation, $r = .135$, between $\mu$ and $\sigma$ for $m_o$. Furthermore, a negative correlation of $r = -.31$ between $\mu$ and $\sigma$ of $s$. Overall this suggests that parameter trade-offs cannot be mainly responsible for the disagreement between individual-level and group-level.

### Latent Class Approach

To further explore the individual differences we estimated the MPT model using a latent-trait approach. This approach tries to distribute the observed individual participants into a number of prespecified classes. To identify the number of classes that are supported by the data, we can use both AIC and BIC. As shown below, both AIC and BIC suggest three classes (for AIC, the second best number of classes is 2, whereas for BIC it is 4).

```{r, fig.width=3,fig.height=3}
hmm_all = read_lines("models/hmm_all.txt")
hmm_model_sel = tibble(classes = factor(1:7),
  AIC = as.numeric(str_extract(hmm_all[str_starts(hmm_all, pattern = "AIC:")],
                               "\\d+.\\d+")),
  BIC = as.numeric(str_extract(hmm_all[str_starts(hmm_all, pattern = "BIC:")],
                               "\\d+.\\d+")))

hmm_model_sel %>% 
  gather("IC", "value", -classes) %>% 
  ggplot(aes(x = classes, y = value, 
             group = IC, shape = IC, linetype = IC)) +
  geom_line() +
  geom_point() 
```

The following shows the class weights for the 3 classes with 95% CIs. 

```{r, fig.width=5,fig.height=3}
class_weights = read.table(text = "class mean x1 lower upper x2
    1            0.488536  [ 0.425518 0.551554 ] 
    2            0.256105  [ 0.256105 0.256105 ] 
    3            0.255359  [ 0.255359 0.255359 ] ",
    header = TRUE) %>% 
  select(-x1, -x2)

ggplot(data = class_weights,
       mapping = aes(x = factor(class),
                     y = mean,
                     ymin = lower,
                     ymax = upper)) +
  geom_pointrange() +
  coord_cartesian(ylim = c(0, 0.6))
```

```{r}
prob_all_3 = read.table(text = "Class1  Class2  Class3
    1  0.778342  0.221118  0.000539
    2  0.933240  0.066713  0.000047
    3  0.993203  0.005675  0.001122
    4  0.111495  0.000016  0.888488
    5  0.995100  0.004091  0.000809
    6  0.988116  0.011810  0.000074
    7  0.608450  0.391549  0.000001
    8  0.909574  0.090363  0.000063
    9  0.021995  0.978005  0.000000
   10  0.002698  0.997302  0.000000
   11  0.933240  0.066713  0.000047
   12  0.949402  0.029961  0.020637
   13  0.021995  0.978005  0.000000
   14  0.669720  0.001689  0.328591
   15  0.005985  0.000001  0.994015
   16  0.005985  0.000001  0.994015
   17  0.998627  0.000494  0.000879
   18  0.157501  0.842499  0.000000
   19  0.005985  0.000001  0.994015
   20  0.669720  0.001689  0.328591
   21  0.998627  0.000494  0.000879
   22  0.219580  0.780413  0.000007
   23  0.980709  0.002679  0.016612
   24  0.002698  0.997302  0.000000
   25  0.831118  0.156878  0.012003
   26  0.005985  0.000001  0.994015
   27  0.032741  0.967258  0.000001
   28  0.002698  0.997302  0.000000
   29  0.005985  0.000001  0.994015
   30  0.005985  0.000001  0.994015
   31  0.827949  0.172047  0.000004
   32  0.157501  0.842499  0.000000
   33  0.778342  0.221118  0.000539
   34  0.973390  0.003696  0.022914
   35  0.627250  0.372721  0.000029
   36  0.716448  0.282862  0.000690
   37  0.111495  0.000016  0.888488
   38  0.157501  0.842499  0.000000
   39  0.953707  0.045298  0.000995
   40  0.168368  0.831624  0.000007
   41  0.990579  0.007866  0.001555
   42  0.002698  0.997302  0.000000
   43  0.005985  0.000001  0.994015
   44  0.909574  0.090363  0.000063
   45  0.015925  0.984075  0.000000
   46  0.082817  0.000017  0.917166
   47  0.998093  0.000686  0.001221
   48  0.527891  0.472108  0.000001
   49  0.527891  0.472108  0.000001
   50  0.953707  0.045298  0.000995
   51  0.082817  0.000017  0.917166
   52  0.953707  0.045298  0.000995
   53  0.700474  0.299502  0.000023
   54  0.745667  0.011250  0.243082
   55  0.669720  0.001689  0.328591
   56  0.993203  0.005675  0.001122
   57  0.983560  0.016337  0.000103
   58  0.119625  0.000144  0.880231
   59  0.005985  0.000001  0.994015
   60  0.005985  0.000001  0.994015
   61  0.652889  0.000198  0.346913
   62  0.082817  0.000017  0.917166
   63  0.005985  0.000001  0.994015
   64  0.005985  0.000001  0.994015
   65  0.997074  0.002769  0.000157
   66  0.652889  0.000198  0.346913
   67  0.778342  0.221118  0.000539
   68  0.002698  0.997302  0.000000
   69  0.219580  0.780413  0.000007
   70  0.981669  0.000323  0.018008
   71  0.219580  0.780413  0.000007
   72  0.980709  0.002679  0.016612")

di3 = bind_cols(di, prob_all_3)

di3 %>% 
  select(participant, Class1:Class3) %>% 
  gather("class", "prob", -participant) %>% 
  group_by(participant) %>% 
  summarize(min_prob = max(prob)) %>% 
  {psych::describe(.$min_prob)} %>% 
  print_table()
```

We then add the individuals' posterior probability of class membership to the existing data. Those show a relatively clear pattern, the lowest class membership probability is 0.53 with mean and median at around .9.

```{r}
d3 = di3 %>% 
  gather("class", "prob", Class1:Class3) %>% 
  group_by(participant, age_group, months, age2) %>% 
  summarise(class = class[which.max(prob)],
            prob_sel = max(prob))

with(d3, table(age_group, class))

```

If we simply classify participants based on their maximum posterior probability of class membership, we see that the largest class, class 1, has almost equally many members from age 4 and age 5, with a slight majority for age 4. Additionally, still 8 6-year olds are classified into class 1. Class 2 consists of an almost equal amount of members from all age-groups, with only 6-year olds, slightly under-represented. Class 3 finally laregly consists of 6-year olds. 

The following figure shows the age in month as a function of class membership. This figure clearly shows that age increases monotonically when going from class 1 to class 3.


```{r, fig.width=6, fig.height=3.5}
ggplot(d3, aes(x = class, y = age2)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, height = 0) 

```

Finally, we inspect the parameter estimates as a function of class.

```{r, fig.width=5, fig.height=4}
class_params = read.table(text = "parameter x1 estimate_c1 x2 lower_c1 upper_c1 x3 estimate_c2 x21 lower_c2 upper_c2 x23 estimate_c3 x32 lower_c3 upper_c3 x33
        m_o = 0.600808  [  0.066186  1.135430 ]  1.000000  [  0.859549  1.140451 ]  1.000000  [  0.937371  1.062629 ]
          m_t = 0.581545  [  0.105257  1.057833 ]  0.000000  [ -0.146289  0.146289 ]  1.000000  [  0.936982  1.063018 ]
            s = 1.000000  [ -4.257337  6.257337 ]  0.666724  [  0.385543  0.947906 ]  0.883616  [  0.723221  1.044011 ]
 ", header = TRUE) %>% 
  select(-starts_with("x")) %>% 
  gather("what", "value", -parameter) %>% 
  separate("what", c("measure", "class")) %>% 
  spread(measure, value)

class_params %>% 
  mutate_at(vars("estimate", "lower", "upper"),
            ~ifelse(. < -0.1, -0.1,
                    ifelse(. > 1.1, 1.1, .))) %>% 
  ggplot(aes(x = fct_rev(factor(class)),
             y = estimate,
             ymin = lower,
             ymax = upper)) +
  geom_pointrange() +
  facet_wrap(~parameter) +
  scale_y_continuous(breaks = c(0, 0.5, 1)) +
  coord_flip() +
  labs(x = "class") +
  theme(panel.grid.major.x = element_line(),
        panel.grid.minor.x = element_line())
```

This shows that for class 1, all parameters are estimated with extreme imprecision. For classes 2 and 3, we see very high levels of $m_o$, a mirror pattern for $m_t$, and a monotonic increase of $s$, which is likely clearly above .5 for both classes.

## Plots

### Stacked bar chart

```{r fig.height=6, fig.width=8}
df.plot = df.exp2 %>% 
  mutate(answer = factor(answer,
                         labels = c("correct",
                                    "match origin",
                                    "match trajectory",
                                    "match neither")),
         question_index = str_c(outcome_actual, "\n",
                                outcome_counterfactual, "\n",
                                collision),
         question_index = as.factor(question_index)) %>% 
  filter(collision == "collision") %>% 
  count(age_group, answer) %>% 
  group_by(age_group) %>% 
  mutate(proportion = n / sum(n),
         label = str_c(round(proportion, 2) * 100, "%")) %>% 
  ungroup()

df.text = df.exp2 %>% 
  distinct(participant, age_group) %>% 
  count(age_group) %>% 
  mutate(x = 4:6,
         y = 1.07,
         label = str_c("n = ", n))

ggplot(data = df.plot, 
       mapping = aes(x = age_group,
                     y = proportion,
                     fill = answer)) + 
  geom_bar(stat = "identity",
           position = position_fill(reverse = TRUE),
           color = "black") +
  geom_hline(yintercept = 0.25,
             linetype = 2,
             size = 1,
             color = "gray20") +
  geom_text(mapping = aes(label = label),
            color = "black",
            # fontface = "bold",
            size = 5,
            position = position_stack(vjust = .5,
                                      reverse = TRUE)) +
  annotate(geom = "text",
           x = df.text$x,
           y = df.text$y,
           label = df.text$label,
           size = 8) + 
  annotate(geom = "text",
           x = rep(6.5, 4),
           y = df.plot %>% 
             filter(age_group == 6) %>% 
             select(proportion) %>% 
             mutate(cum_prop = cumsum(proportion),
                    lag = lag(cum_prop, 1, default = 0),
                    y = (cum_prop + lag) / 2) %>% 
             pull(y),
           label = c("correct",
                     "match origin",
                     "match trajectory",
                     "match neither"),
           hjust = 0,
           size = 7) + 
  labs(y = "% of responses") +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     labels = str_c(seq(0, 100, 25), "%"),
                     expand = expansion(mult = 0)) +
  scale_x_continuous(breaks = 4:6,
                     labels = str_c(4:6, " years"),
                     expand = expansion(add = 0.1)) + 
  coord_cartesian(clip = "off") + 
  scale_fill_manual(values = c(rgb(252, 0, 8, maxColorValue = 255),
                               rgb(254, 255, 13, maxColorValue = 255),
                               rgb(135, 0, 197, maxColorValue = 255),
                               rgb(53, 91, 183, maxColorValue = 255))) +
  theme(text = element_text(size = 30),
        legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        legend.title = element_blank(),
        plot.margin = margin(t = 0.5,
                             r = 5,
                             b = 0,
                             l = 0,
                             unit = "cm"))

ggsave("../figures//experiment2_bars.pdf",
       width = 8,
       height = 6)
```

Let's also look at these results split up by singly-determined and over-determined
```{r fig.height=6, fig.width=8}
df.plot_singly_determined = df.exp2 %>% 
  mutate(answer = factor(answer,
                         labels = c("correct",
                                    "match origin",
                                    "match trajectory",
                                    "match neither")),
         question_index = str_c(outcome_actual, "\n",
                                outcome_counterfactual, "\n",
                                collision),
         question_index = as.factor(question_index)) %>% 
  filter(collision == "collision") %>% 
  filter(outcome_counterfactual=="different") %>%
  count(age_group, answer) %>%
  group_by(age_group) %>% 
  mutate(proportion = n / sum(n),
         label = str_c(round(proportion, 2) * 100, "%")) %>% 
  ungroup()


ggplot(data = df.plot_singly_determined, 
       mapping = aes(x = age_group,
                     y = proportion,
                     fill = answer)) + 
  ggtitle("Responses to singly-determined items") +
  geom_bar(stat = "identity",
           position = position_fill(reverse = TRUE),
           color = "black") +
  geom_hline(yintercept = 0.25,
             linetype = 2,
             size = 1,
             color = "gray20") +
  geom_text(mapping = aes(label = label),
            color = "black",
            # fontface = "bold",
            size = 5,
            position = position_stack(vjust = .5,
                                      reverse = TRUE)) +
  annotate(geom = "text",
           x = df.text$x,
           y = df.text$y,
           label = df.text$label,
           size = 8) + 
  annotate(geom = "text",
           x = rep(6.5, 4),
           y = df.plot %>% 
             filter(age_group == 6) %>% 
             select(proportion) %>% 
             mutate(cum_prop = cumsum(proportion),
                    lag = lag(cum_prop, 1, default = 0),
                    y = (cum_prop + lag) / 2) %>% 
             pull(y),
           label = c("correct",
                     "match origin",
                     "match trajectory",
                     "match neither"),
           hjust = 0,
           size = 7) + 
  labs(y = "% of responses") +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     labels = str_c(seq(0, 100, 25), "%"),
                     expand = expansion(mult = 0)) +
  scale_x_continuous(breaks = 4:6,
                     labels = str_c(4:6, " years"),
                     expand = expansion(add = 0.1)) + 
  coord_cartesian(clip = "off") + 
  scale_fill_manual(values = c(rgb(252, 0, 8, maxColorValue = 255),
                               rgb(254, 255, 13, maxColorValue = 255),
                               rgb(135, 0, 197, maxColorValue = 255),
                               rgb(53, 91, 183, maxColorValue = 255))) +
  theme(text = element_text(size = 30),
        legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        legend.title = element_blank(),
        plot.margin = margin(t = 0.5,
                             r = 5,
                             b = 0,
                             l = 0,
                             unit = "cm"))

ggsave("../figures//experiment2__singlydetermined_bars.pdf",
       width = 8,
       height = 6)

```
```{r fig.height=6, fig.width=8}
df.plot_over_determined = df.exp2 %>% 
  mutate(answer = factor(answer,
                         labels = c("correct",
                                    "match origin",
                                    "match trajectory",
                                    "match neither")),
         question_index = str_c(outcome_actual, "\n",
                                outcome_counterfactual, "\n",
                                collision),
         question_index = as.factor(question_index)) %>% 
  filter(collision == "collision") %>% 
  filter(outcome_counterfactual=="same") %>%
  count(age_group, answer) %>%
  group_by(age_group) %>% 
  mutate(proportion = n / sum(n),
         label = str_c(round(proportion, 2) * 100, "%")) %>% 
  ungroup()


ggplot(data = df.plot_over_determined, 
       mapping = aes(x = age_group,
                     y = proportion,
                     fill = answer)) + 
  ggtitle("Responses to over-determined items") +
  geom_bar(stat = "identity",
           position = position_fill(reverse = TRUE),
           color = "black") +
  geom_hline(yintercept = 0.25,
             linetype = 2,
             size = 1,
             color = "gray20") +
  geom_text(mapping = aes(label = label),
            color = "black",
            # fontface = "bold",
            size = 5,
            position = position_stack(vjust = .5,
                                      reverse = TRUE)) +
  annotate(geom = "text",
           x = df.text$x,
           y = df.text$y,
           label = df.text$label,
           size = 8) + 
  annotate(geom = "text",
           x = rep(6.5, 4),
           y = df.plot %>% 
             filter(age_group == 6) %>% 
             select(proportion) %>% 
             mutate(cum_prop = cumsum(proportion),
                    lag = lag(cum_prop, 1, default = 0),
                    y = (cum_prop + lag) / 2) %>% 
             pull(y),
           label = c("correct",
                     "match origin",
                     "match trajectory",
                     "match neither"),
           hjust = 0,
           size = 7) + 
  labs(y = "% of responses") +
  scale_y_continuous(breaks = seq(0, 1, 0.25),
                     labels = str_c(seq(0, 100, 25), "%"),
                     expand = expansion(mult = 0)) +
  scale_x_continuous(breaks = 4:6,
                     labels = str_c(4:6, " years"),
                     expand = expansion(add = 0.1)) + 
  coord_cartesian(clip = "off") + 
  scale_fill_manual(values = c(rgb(252, 0, 8, maxColorValue = 255),
                               rgb(254, 255, 13, maxColorValue = 255),
                               rgb(135, 0, 197, maxColorValue = 255),
                               rgb(53, 91, 183, maxColorValue = 255))) +
  theme(text = element_text(size = 30),
        legend.position = "none",
        axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        legend.title = element_blank(),
        plot.margin = margin(t = 0.5,
                             r = 5,
                             b = 0,
                             l = 0,
                             unit = "cm"))

ggsave("../figures//experiment2_overdetermined_bars.pdf",
       width = 8,
       height = 6)

```
### Group-level parameteres

The following plot shows the posterior distribution of the group-level parameters, separated by age-group. 

```{r, fig.height=8, fig.width=4}

mus <- gather_draws(fit_bayes_all$runjags, mu[i]) %>% 
    mutate(
      parameter = factor(
        i, 
        levels = c(3, 1, 2), 
        labels = c("italic(s)", "italic(m[o])", "italic(m[t])"))
    ) %>% 
    rename(mean = .value) %>% 
    ungroup() %>% 
    select(-i, -.variable)

pars = gather_draws(fit_bayes_all$runjags, 
                     `factor_.*_age`[i], regex = TRUE) %>% 
  ungroup() %>% 
  mutate(age_group = factor(age_groups[i], level = age_groups),
         #estimate = pnorm(.value),
         parameter = factor(
           str_replace(str_replace(.variable, "factor_", ""), "_age", ""), 
           levels = c("s", "m_o", "m_t"), 
           labels = c("italic(s)", "italic(m[o])", "italic(m[t])"))) %>% 
  mutate(age_group2 = factor(age_group, levels = rev(levels(di$age))))
pars <- left_join(pars, mus) %>% 
    mutate(estimate = pnorm(.value + mean))
ggplot(pars, aes(x = estimate,
                 y = age_group, 
                 # fill = ..x..,
                 height = ..density..)) +
  geom_density_ridges(scale = 2.0,
                      panel_scaling = TRUE, 
                      rel_min_height = 0.001,
                      size = 1.25, 
                      stat = "density",
                      bw = "nrd", 
                      fill = "lightblue",
                      alpha = 0.7) +
  coord_cartesian(xlim = c(0, 1), ylim = c(1, 5.1)) +
  facet_wrap(~parameter,
             ncol = 1,
             labeller = label_parsed) +
  scale_y_discrete(expand = c(0.01, 0)) +
  scale_x_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), 
                     labels = c("0", "", "0.5", "", "1")) +
  labs(y = "age group", x = "group-level\nparameter estimate") + 
  # theme_ridges(center = TRUE) +
  theme(panel.grid.minor.x = element_line(),
        panel.grid.major.y = element_line())

ggsave("../figures//experiment2_estimates.pdf",
       width = 4,
       height = 8)
```

# Experiment 3 

## Read in data 

```{r, message=F}
df.exp3 = read_csv("../data/experiment3_data.csv") %>% 
  clean_names() %>% 
  mutate(train_acc = ifelse(train_q == 2, 1, 0),
         test_acc = ifelse(test == 2, 1, 0)) %>% 
  select(participant = snum, loc:test_acc)
```


## Stats 

### Binomial test  

```{r}
df.results = df.exp3 %>%
  summarize(n = n(),
            n_correct = sum(test_acc == 1))

binom.test(x = df.results$n_correct,
           n = df.results$n) %>% 
  tidy() %>% 
  print_table(digits = 3)
```

### MPT model analysis

```{r, message=FALSE}
data = read.csv("../data/experiment3_data_individual.csv")
# 1 = incorrect 2 = correct.
data.coded = data %>%
  mutate(TrainAcc = ifelse(TrainQ == 2, 1, 0),
         TrainErr = ifelse(TrainQ == 1, 1, 0),
         TestAcc = ifelse(Test == 2, 1, 0),
         TestErr = ifelse(Test == 1, 1, 0))

data.agg = data.coded %>%
  summarize(corr=sum(TestAcc),err=sum(TestErr))
```

### MPT analysis

First, we get a simple point estimate.

```{r, message=FALSE}
fit1 = fit.mpt(data.agg, "models/mpt_exp3.txt")

fit1$goodness.of.fit

fit1$parameters
# raw parameter estimate of $s$ is .333
```

However, to be comparable with the previous analyses, we need to conduct a Bayesian analysis of the aggregated data. Because we only have one trial per child, we opted for the Bayesian model on the aggregated data in this case. This assumes that individual variability is consistent with the variability of a multinomial distribution.

```{r, message=FALSE}
data.agg2 = data.coded %>%
  summarize(TestAcc = sum(TestAcc),
            TestErr = sum(TestErr))

smpt = simpleMPT("models/model3.eqn",
                 data.agg2, 
                 restrictions = list("g=0.5"), 
                 n.chains = 4)
summary(smpt)
str(smpt, 2)
head(smpt$runjags$mcmc)
samps = gather_draws(smpt$runjags$mcmc, mean)
```

Here are the mode and mean of the estimate of *s*, and their 80% HDIs, based on this analysis.

Mode (reported, for comparison w/Exp 2 4-year-olds):
```{r}
samps %>% 
  mode_hdcih(.value,
             .width = c(0.80))
```
Mean:
```{r}
samps %>% 
  mean_hdcih(.value,
             .width = c(0.80))
```

## Plots

Density plot of the the estimate.

```{r, fig.width = 9, fig.height = 4}
ggplot(samps,
       aes(x = .value,
           y = .variable, 
           height = ..density..)) +
  geom_density_ridges(scale = 2.0,
                      panel_scaling = TRUE, 
                      rel_min_height = 0.001,
                      size = 1.25, 
                      stat = "density",
                      bw = "nrd", 
                      fill = "lightblue",
                      alpha = 0.7) +
  coord_cartesian(xlim = c(0, 1), ylim = c(1, 7.1)) +
  scale_y_discrete(expand = c(0.01, 0)) +
  scale_x_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1), 
                     labels = c("0", "", "0.5", "", "1")) +
  labs(y = element_blank(), x = "group-level\nparameter estimate") + 
  theme(panel.grid.minor.x = element_line(),
        panel.grid.major.y = element_line())

ggsave("../figures/experiment3_estimate.pdf",
       width = 4,
       height = 4)
```

# Session info

```{r}
sessionInfo()
```

